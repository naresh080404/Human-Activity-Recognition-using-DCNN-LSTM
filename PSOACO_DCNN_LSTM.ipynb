{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"UCI HAR Dataset/train/y_train.txt\", delim_whitespace=True, header=None).values.ravel()\n",
    "y_test = pd.read_csv(\"UCI HAR Dataset/test/y_test.txt\", delim_whitespace=True, header=None).values.ravel()\n",
    "final_df1 = pd.read_csv(\"geneticalgooutput.csv\")\n",
    "final_df1 = final_df1.iloc[:, :89]\n",
    "y_combined_df = pd.concat([pd.Series(y_train), pd.Series(y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d0014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(selected_features, ants):\n",
    "    num_features = len(selected_features)\n",
    "    if num_features == 0:\n",
    "        return np.zeros(len(ants[0]))\n",
    "    similarity = np.sum(ants[:, selected_features], axis=0) / num_features\n",
    "    similarity = np.pad(similarity, (0, len(ants[0]) - num_features), 'constant')\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e110ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aco_feature_selection(num_features, num_ants, num_iterations_aco, X, y):\n",
    "    pheromones = np.ones(num_features)\n",
    "    selected_features = []\n",
    "    temporary=[i for i in range(10299)]\n",
    "    ants = X[temporary,:]\n",
    "    ant_fitness1 = np.array([np.mean(ant) for ant in ants])\n",
    "    for aco_iteration in range(num_iterations_aco):\n",
    "        ant_fitness=[]\n",
    "        random_indices=np.random.choice([i for i in range(10299)],size=num_ants)\n",
    "        for i in random_indices:\n",
    "            ant_fitness.append(ant_fitness1[i])\n",
    "\n",
    "        best_ant_index = np.argmax(ant_fitness)\n",
    "\n",
    "\n",
    "\n",
    "        if 0 <= best_ant_index < 10299:\n",
    "            best_ant_position = ants[best_ant_index].copy()\n",
    "\n",
    "            similarity = calculate_similarity(selected_features, ants)\n",
    "            for feature_i in range(num_features):\n",
    "                pheromones[feature_i] += ant_fitness[best_ant_index] * (1 - similarity[feature_i])\n",
    "\n",
    "            pheromone_probs = np.exp(pheromones) / np.sum(np.exp(pheromones))\n",
    "\n",
    "            selected_feature = np.random.choice(np.arange(num_features), p=pheromone_probs)\n",
    "            selected_features.append(selected_feature)  # Increment by 1 to match original feature indices\n",
    "\n",
    "            print(f\"ACO Iteration {aco_iteration + 1}/{num_iterations_aco}: Selected feature {selected_feature}\")\n",
    "        else:\n",
    "            print(\"Invalid best_ant_index:\", best_ant_index)\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b90d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df1\n",
    "y = y_combined_df\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_final = X_scaled[:]\n",
    "\n",
    "selected_features_aco = aco_feature_selection(X_final.shape[1], num_ants=200, num_iterations_aco=47, X=X_final, y=y)\n",
    "selected_features_aco\n",
    "\n",
    "X_final = X_scaled[:,selected_features_aco]\n",
    "\n",
    "X_final=pd.DataFrame(X_final)\n",
    "X_final.head(10299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from niapy.problems import Problem\n",
    "from niapy.task import Task\n",
    "from niapy.algorithms.basic import ParticleSwarmOptimization\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMFeatureSelection(Problem):\n",
    "    def __init__(self, X_train, y_train, alpha=0.99):\n",
    "        super().__init__(dimension=X_train.shape[1], lower=[0] * X_train.shape[1], upper=[1] * X_train.shape[1])\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        selected = x > 0.5\n",
    "        num_selected = selected.sum()\n",
    "        if num_selected == 0:\n",
    "            return 1.0\n",
    "        accuracy = cross_val_score(SVC(), self.X_train[:, selected], self.y_train, cv=2, n_jobs=-1).mean()\n",
    "        score = 1 - accuracy\n",
    "        num_features = self.X_train.shape[1]\n",
    "        return self.alpha * score + (1 - self.alpha) * (num_selected / num_features)\n",
    "\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_final)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_encoded = np.ravel(y_encoded)\n",
    "X_tensor = torch.FloatTensor(X_scaled.astype('float32'))\n",
    "y_tensor = torch.LongTensor(y_encoded).ravel()\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "problem = SVMFeatureSelection(X_train, y_train)\n",
    "task = Task(problem, max_iters=100)\n",
    "algorithm = ParticleSwarmOptimization(population_size=10, seed=1234)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "\n",
    "\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(map(str, np.where(selected_features)[0])))\n",
    "\n",
    "model_selected = SVC()\n",
    "model_all = SVC()\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print('Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "\n",
    "model_all.fit(X_train, y_train)\n",
    "print('All Features Accuracy:', model_all.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_nparray=np.array(X_final)\n",
    "X_temparr=X_final_nparray[:,selected_features]\n",
    "X_temp=pd.DataFrame(X_final_nparray[:,selected_features])\n",
    "X_temp.head(10299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21705e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a314019",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_combined_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_tensor = torch.FloatTensor(X_temparr)\n",
    "y_tensor = torch.LongTensor(y_combined_encoded)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.lstm1 = nn.LSTM(input_size=128, hidden_size=hidden_size, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x.unsqueeze(1)))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model1 = HybridModel(input_size=20, hidden_size=64, num_classes=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    total_loss = 0.0\n",
    "    correct_train_predictions = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train_samples += labels.size(0)\n",
    "        correct_train_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    train_epoch_loss = total_loss / len(train_loader)\n",
    "    train_epoch_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accuracies.append(train_epoch_accuracy)\n",
    "\n",
    "    model1.eval()\n",
    "    correct_test_predictions = 0\n",
    "    total_test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model1(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_test_samples += labels.size(0)\n",
    "            correct_test_predictions += (preds == labels).sum().item()\n",
    "\n",
    "    test_epoch_accuracy = correct_test_predictions / total_test_samples\n",
    "\n",
    "    test_accuracies.append(test_epoch_accuracy)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_epoch_loss:.4f}, Train Accuracy: {train_epoch_accuracy:.4f}, \"\n",
    "          f\"Test Accuracy: {test_epoch_accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "activity_names = ['Walking', 'Walking Upstairs', 'Walking Downstairs', 'Sitting', 'Standing', 'Laying']\n",
    "\n",
    "adjusted_all_labels = [label - 1 for label in all_labels]\n",
    "adjusted_all_preds = [pred - 1 for pred in all_preds]\n",
    "\n",
    "conf_matrix = confusion_matrix(adjusted_all_labels, adjusted_all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=activity_names, yticklabels=activity_names, annot_kws={\"size\": 10})\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
